{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa01e0e4",
   "metadata": {},
   "source": [
    "### Step 1 : Combine all the paraquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa291c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaForMaskedLM, AutoTokenizer, XLMRobertaTokenizerFast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed3cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/krrish/Desktop/Programming/slm-distill/dataset/hin/train\"\n",
    "TEST_DATA = \"/home/krrish/Desktop/Programming/slm-distill/dataset/hin/train/data-0.parquet\"\n",
    "TEACHER_PATH = \"xlm-roberta-base\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1ccba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = XLMRobertaForMaskedLM.from_pretrained(model_path).to(device)\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, return_logits=True):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids.to(self.device),\n",
    "                attention_mask=attention_mask.to(self.device) if attention_mask is not None else None,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            \n",
    "        if return_logits:\n",
    "            return outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            return outputs\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def get_num_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a797106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaForMaskedLM, XLMRobertaConfig\n",
    "\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 250002,\n",
    "        hidden_size: int = 256,\n",
    "        num_hidden_layers: int = 6,\n",
    "        num_attention_heads: int = 8,\n",
    "        intermediate_size: int = 1024,\n",
    "        max_position_embeddings: int = 512,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_probs_dropout_prob: float = 0.1,\n",
    "        pad_token_id: int = 1,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        use_gradient_checkpointing: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate\n",
    "        assert hidden_size % num_attention_heads == 0, \\\n",
    "            f\"hidden_size ({hidden_size}) must be divisible by num_attention_heads ({num_attention_heads})\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # XLM-RoBERTa Config\n",
    "        config = XLMRobertaConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            intermediate_size=intermediate_size,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "            pad_token_id=pad_token_id,\n",
    "        )\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = XLMRobertaForMaskedLM(config).to(self.device)\n",
    "        \n",
    "        if use_gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        labels=None,\n",
    "        return_logits=True\n",
    "    ):\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            \n",
    "        if labels is not None:\n",
    "            labels = labels.to(self.device)\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        \n",
    "        if return_logits:\n",
    "            return outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            return outputs\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc9c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple dataset class for parquet data\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "\n",
    "class NativeSLMData(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        train_split: float = 0.95,\n",
    "        seed: int = 42,\n",
    "        cache_dir: str = None\n",
    "    ):\n",
    "        cache_path = None\n",
    "        \n",
    "        if cache_dir:\n",
    "            cache_path = os.path.join(\n",
    "                cache_dir, \n",
    "                f\"{os.path.basename(data_path)}_{split}_ml{max_length}\"\n",
    "            )\n",
    "            \n",
    "            if os.path.exists(cache_path):\n",
    "                self.dataset = load_from_disk(cache_path).with_format(\"torch\")\n",
    "                return\n",
    "        \n",
    "        df = pd.read_parquet(data_path)\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        \n",
    "        split_dataset = dataset.train_test_split(train_size=train_split, seed=seed)\n",
    "        dataset = split_dataset['train'] if split == 'train' else split_dataset['test']\n",
    "        \n",
    "        self.dataset = dataset.map(\n",
    "            \n",
    "            lambda x: tokenizer(\n",
    "                x[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_special_tokens_mask=True,\n",
    "            ),\n",
    "            batched=True,\n",
    "            batch_size=1024,\n",
    "            remove_columns=dataset.column_names,\n",
    "            \n",
    "        ).with_format(\"torch\")\n",
    "        \n",
    "        # Save to cache\n",
    "        if cache_path:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            print(f\"Saving to cache: {cache_path}\")\n",
    "            self.dataset.save_to_disk(cache_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9c1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "teacher = TeacherModel(model_path=TEACHER_PATH, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_PATH, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8207a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278295186, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.get_num_parameters(), teacher.get_num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbb9fb",
   "metadata": {},
   "source": [
    "Okay works that teacher doesn't have any trainable parameters. Check passed âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a29627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f087e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = StudentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6859cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69187474, 69187474)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "student.get_num_parameters(), student.get_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NativeSLMData(\n",
    "    data_path=DATA_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128,\n",
    "    split=\"train\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm-distill (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
