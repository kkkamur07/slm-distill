{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a17c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/krrish/Desktop/Programming/slm-distill\n",
      "Python path: ['/home/krrish/Desktop/Programming/slm-distill', '/usr/lib/python311.zip']\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Setup Python Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path: {sys.path[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac8f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERPLEXITY EVALUATION\n",
      "======================================================================\n",
      "Checkpoint:  /home/krrish/Desktop/Programming/slm-distill/outputs/final_model\n",
      "Data:        /home/krrish/Desktop/Programming/slm-distill/data/hin/data-0.parquet\n",
      "Device:      cuda\n",
      "======================================================================\n",
      "\n",
      "[1] Checking files...\n",
      "✓ Checkpoint found\n",
      "✓ Data file found\n",
      "\n",
      "[2] Loading model and tokenizer...\n",
      "✓ Model loaded: 33.1M parameters\n",
      "✓ Tokenizer loaded: vocab size = 250002\n",
      "\n",
      "[3] Loading evaluation data...\n",
      "\n",
      "Loading data from parquet...\n",
      "Data path: /home/krrish/Desktop/Programming/slm-distill/data/hin/data-0.parquet\n",
      "✓ Loaded 174,763 examples\n",
      "Columns: ['doc_id', 'text', 'type']\n",
      "✓ Train: 166,024 examples (95.0%)\n",
      "✓ Eval: 8,739 examples (5.0%)\n",
      "Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f12c64665a643e8856357cc1a48ffc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/166024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5df5bd6a0614ef2af2477879871d368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval:   0%|          | 0/8739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train batches: 5,189\n",
      "✓ Eval batches: 274\n",
      "✓ Approx train tokens: 21.3M\n",
      "✓ Loaded 274 batches\n",
      "\n",
      "[4] Computing perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/274 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 274/274 [00:10<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 274 batches\n",
      "\n",
      "[5] Calculating metrics...\n",
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "Total tokens:           163,553\n",
      "Total batches:          274\n",
      "Average loss:           3.0060\n",
      "Perplexity:             20.21\n",
      "======================================================================\n",
      "\n",
      "[6] Batch Loss Statistics:\n",
      "  Mean:                 3.0041\n",
      "  Median:               2.9983\n",
      "  Std Dev:              0.1870\n",
      "  Min:                  2.5068\n",
      "  Max:                  3.6110\n",
      "  Q1 (25%):             2.8684\n",
      "  Q3 (75%):             3.1294\n",
      "\n",
      "[7] Token Count Statistics:\n",
      "  Mean tokens/batch:    597\n",
      "  Total tokens:         163,553\n",
      "  Min batch size:       55\n",
      "  Max batch size:       661\n",
      "\n",
      "[8] Evaluating sample sentences...\n",
      "  PPL:   45.00 | मैं स्कूल जाता हूँ।\n",
      "  PPL:   25.60 | आज मौसम बहुत अच्छा है।\n",
      "  PPL:   24.33 | शिक्षा जीवन का आधार है।\n",
      "  PPL:    8.76 | भारतीय संविधान दुनिया का सबसे बड़ा लिखित संविधान ह\n",
      "  PPL:   13.63 | जलवायु परिवर्तन एक गंभीर समस्या है।\n",
      "\n",
      "✓ Average sample perplexity: 23.46\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "✓ Checkpoint:           /home/krrish/Desktop/Programming/slm-distill/outputs/final_model\n",
      "✓ Model size:           33.1M parameters\n",
      "✓ Data file:            /home/krrish/Desktop/Programming/slm-distill/data/hin/data-0.parquet\n",
      "✓ Tokens evaluated:     163,553\n",
      "✓ Batches:              274\n",
      "✓ Dataset perplexity:   20.21\n",
      "✓ Sample avg perplexity: 23.46\n",
      "======================================================================\n",
      "\n",
      "✅ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Comprehensive Perplexity Evaluation\n",
    "\n",
    "import torch\n",
    "from transformers import XLMRobertaForMaskedLM, AutoTokenizer\n",
    "from src.data.data import prepare_datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "checkpoint = \"/home/krrish/Desktop/Programming/slm-distill/outputs/final_model\"\n",
    "data_path = \"/home/krrish/Desktop/Programming/slm-distill/data/hin/data-0.parquet\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32\n",
    "max_length = 128\n",
    "train_split = 0.95\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERPLEXITY EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Checkpoint:  {checkpoint}\")\n",
    "print(f\"Data:        {data_path}\")\n",
    "print(f\"Device:      {device}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 1. VERIFY FILES EXIST\n",
    "# ============================================================\n",
    "print(\"\\n[1] Checking files...\")\n",
    "\n",
    "if not os.path.exists(checkpoint):\n",
    "    print(f\"❌ Error: Checkpoint not found at {checkpoint}\")\n",
    "    print(\"Available checkpoints:\")\n",
    "    if os.path.exists(\"outputs\"):\n",
    "        for item in os.listdir(\"outputs\"):\n",
    "            print(f\"  - {item}\")\n",
    "else:\n",
    "    print(f\"✓ Checkpoint found\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"❌ Error: Data not found at {data_path}\")\n",
    "else:\n",
    "    print(f\"✓ Data file found\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD MODEL & TOKENIZER\n",
    "# ============================================================\n",
    "print(\"\\n[2] Loading model and tokenizer...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "    model = XLMRobertaForMaskedLM.from_pretrained(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    print(f\"✓ Model loaded: {num_params:.1f}M parameters\")\n",
    "    print(f\"✓ Tokenizer loaded: vocab size = {len(tokenizer)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# 3. LOAD EVALUATION DATA\n",
    "# ============================================================\n",
    "print(\"\\n[3] Loading evaluation data...\")\n",
    "\n",
    "try:\n",
    "    _, eval_loader = prepare_datasets(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size,\n",
    "        train_split=train_split,\n",
    "        num_workers=2\n",
    "    )\n",
    "    print(f\"✓ Loaded {len(eval_loader)} batches\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# 4. COMPUTE PERPLEXITY\n",
    "# ============================================================\n",
    "print(\"\\n[4] Computing perplexity...\")\n",
    "\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "batch_losses = []\n",
    "batch_token_counts = []\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(eval_loader, desc=\"Evaluating\")):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Count masked tokens\n",
    "            num_tokens = (labels != -100).sum().item()\n",
    "            batch_loss = outputs.loss.item()\n",
    "            \n",
    "            # Accumulate\n",
    "            total_loss += batch_loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_token_counts.append(num_tokens)\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del outputs, input_ids, attention_mask, labels\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"✓ Processed {len(eval_loader)} batches\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during evaluation: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================\n",
    "# 5. CALCULATE METRICS\n",
    "# ============================================================\n",
    "print(\"\\n[5] Calculating metrics...\")\n",
    "\n",
    "avg_loss = total_loss / total_tokens\n",
    "perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total tokens:           {total_tokens:,}\")\n",
    "print(f\"Total batches:          {len(eval_loader)}\")\n",
    "print(f\"Average loss:           {avg_loss:.4f}\")\n",
    "print(f\"Perplexity:             {perplexity:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================\n",
    "# 6. DETAILED STATISTICS\n",
    "# ============================================================\n",
    "print(\"\\n[6] Batch Loss Statistics:\")\n",
    "\n",
    "print(f\"  Mean:                 {np.mean(batch_losses):.4f}\")\n",
    "print(f\"  Median:               {np.median(batch_losses):.4f}\")\n",
    "print(f\"  Std Dev:              {np.std(batch_losses):.4f}\")\n",
    "print(f\"  Min:                  {np.min(batch_losses):.4f}\")\n",
    "print(f\"  Max:                  {np.max(batch_losses):.4f}\")\n",
    "print(f\"  Q1 (25%):             {np.percentile(batch_losses, 25):.4f}\")\n",
    "print(f\"  Q3 (75%):             {np.percentile(batch_losses, 75):.4f}\")\n",
    "\n",
    "print(\"\\n[7] Token Count Statistics:\")\n",
    "\n",
    "print(f\"  Mean tokens/batch:    {np.mean(batch_token_counts):,.0f}\")\n",
    "print(f\"  Total tokens:         {total_tokens:,}\")\n",
    "print(f\"  Min batch size:       {np.min(batch_token_counts):,}\")\n",
    "print(f\"  Max batch size:       {np.max(batch_token_counts):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. SAMPLE SENTENCE EVALUATION\n",
    "# ============================================================\n",
    "print(\"\\n[8] Evaluating sample sentences...\")\n",
    "\n",
    "hindi_samples = [\n",
    "    \"मैं स्कूल जाता हूँ।\",\n",
    "    \"आज मौसम बहुत अच्छा है।\",\n",
    "    \"शिक्षा जीवन का आधार है।\",\n",
    "    \"भारतीय संविधान दुनिया का सबसे बड़ा लिखित संविधान है।\",\n",
    "    \"जलवायु परिवर्तन एक गंभीर समस्या है।\"\n",
    "]\n",
    "\n",
    "sentence_ppls = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sent in hindi_samples:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        ppl = torch.exp(outputs.loss).item()\n",
    "        sentence_ppls.append(ppl)\n",
    "        print(f\"  PPL: {ppl:7.2f} | {sent[:50]}\")\n",
    "\n",
    "avg_sent_ppl = np.mean(sentence_ppls)\n",
    "print(f\"\\n✓ Average sample perplexity: {avg_sent_ppl:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. FINAL SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Checkpoint:           {checkpoint}\")\n",
    "print(f\"✓ Model size:           {num_params:.1f}M parameters\")\n",
    "print(f\"✓ Data file:            {data_path}\")\n",
    "print(f\"✓ Tokens evaluated:     {total_tokens:,}\")\n",
    "print(f\"✓ Batches:              {len(eval_loader)}\")\n",
    "print(f\"✓ Dataset perplexity:   {perplexity:.2f}\")\n",
    "print(f\"✓ Sample avg perplexity: {avg_sent_ppl:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Return results for further use\n",
    "results = {\n",
    "    \"perplexity\": perplexity,\n",
    "    \"avg_loss\": avg_loss,\n",
    "    \"total_tokens\": total_tokens,\n",
    "    \"batch_losses\": batch_losses,\n",
    "    \"sample_ppls\": sentence_ppls\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm-distill (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
